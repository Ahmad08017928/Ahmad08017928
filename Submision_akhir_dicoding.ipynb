{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQQWshQX4vBD2q3X2zXUQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmad08017928/Ahmad08017928/blob/main/Submision_akhir_dicoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Import Library**"
      ],
      "metadata": {
        "id": "1DVpr_DoK4eg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA93Qn6RF7f8"
      },
      "outputs": [],
      "source": [
        "# Library yang sering digunakan\n",
        "import os, shutil, json\n",
        "import zipfile\n",
        "import random\n",
        "from random import sample\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm as tq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries untuk pemrosesan data gambar\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import skimage\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.exposure import adjust_gamma\n",
        "from skimage.util import random_noise"
      ],
      "metadata": {
        "id": "O8g2Wq5uGqok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries untuk pembangunan model\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, Add, Activation, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
        "# from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout, Input, Add, Activation, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "id": "d5Eg-YhXGsEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "d6zJWTQuGua4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mengkoneksi-kan ke Kaggle**"
      ],
      "metadata": {
        "id": "Q6MjE4aIK_Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan API token di ~/.kaggle/kaggle.json\n",
        "os.makedirs('/root/.kaggle/', exist_ok=True)\n",
        "\n",
        "api_token = {\"username\": \"what6476\", \"key\": \"6eda561fb1794c48b6140e3ade630090\"}\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "# Atur izin\n",
        "os.chmod('/root/.kaggle/kaggle.json', 600)"
      ],
      "metadata": {
        "id": "_ptthbdTGwuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Install Kaggle dan Unzip file yang di tuju**"
      ],
      "metadata": {
        "id": "4YKOCMVlLRq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "J21xYpgrG8Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dan unzip dataset\n",
        "!kaggle datasets download -d tolgadincer/labeled-chest-xray-images\n",
        "!unzip labeled-chest-xray-images.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ3owcE6HAMT",
        "outputId": "64b86869-1ef9-4e7f-bf32-1545bf815e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/tolgadincer/labeled-chest-xray-images\n",
            "License(s): other\n",
            "labeled-chest-xray-images.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  labeled-chest-xray-images.zip\n",
            "replace chest_xray/test/NORMAL/NORMAL-1049278-0001.jpeg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Direktori awal untuk train dan test\n",
        "train_dir = \"chest_xray/train\"\n",
        "test_dir = \"chest_xray/test\"\n",
        "\n",
        "# Direktori baru untuk dataset gabungan\n",
        "combined_dir = \"chest_xray/dataset\""
      ],
      "metadata": {
        "id": "zyehRvIvHH0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat direktori baru untuk dataset gabungan\n",
        "os.makedirs(combined_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "zM_xHGIPHX23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salin file dan folder dari train\n",
        "for category in os.listdir(train_dir):\n",
        "    category_dir = os.path.join(train_dir, category)\n",
        "    if os.path.isdir(category_dir):\n",
        "        shutil.copytree(category_dir, os.path.join(combined_dir, category), dirs_exist_ok=True)\n",
        "\n",
        "# Salin file dan folder dari test\n",
        "for category in os.listdir(test_dir):\n",
        "    category_dir = os.path.join(test_dir, category)\n",
        "    if os.path.isdir(category_dir):\n",
        "        shutil.copytree(category_dir, os.path.join(combined_dir, category), dirs_exist_ok=True)"
      ],
      "metadata": {
        "id": "XVcI7x2hHZ2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Checking**"
      ],
      "metadata": {
        "id": "5FjAjAhILc8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data\n",
        "lung_image = {}\n",
        "\n",
        "# Tentukan path sumber train\n",
        "path = \"chest_xray/\"\n",
        "path_sub = os.path.join(path, \"dataset\")\n",
        "for i in os.listdir(path_sub):\n",
        "    lung_image[i] = os.listdir(os.path.join(path_sub, i))"
      ],
      "metadata": {
        "id": "3sORhNe8IbpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan secara acak 5 gambar di bawah setiap dari 2 kelas dari data.\n",
        "# Anda akan melihat gambar yang berbeda setiap kali kode ini dijalankan.\n",
        "path_sub = \"chest_xray/dataset/\"\n",
        "\n",
        "# Menampilkan secara acak 5 gambar di bawah setiap kelas dari data latih\n",
        "fig, axs = plt.subplots(len(lung_image.keys()), 5, figsize=(15, 15))\n",
        "\n",
        "for i, class_name in enumerate(os.listdir(path_sub)):\n",
        "    images = np.random.choice(lung_image[class_name], 5, replace=False)\n",
        "\n",
        "    for j, image_name in enumerate(images):\n",
        "        img_path = os.path.join(path_sub, class_name, image_name)\n",
        "        img = Image.open(img_path).convert(\"L\")  # Konversi menjadi skala keabuan\n",
        "        axs[i, j].imshow(img, cmap='gray')\n",
        "        axs[i, j].set(xlabel=class_name, xticks=[], yticks=[])\n",
        "\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "UECWVTHjIeuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisikan path sumber\n",
        "lung_path = \"chest_xray/dataset/\"\n",
        "\n",
        "# Buat daftar yang menyimpan data untuk setiap nama file, path file, dan label dalam data\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "\n",
        "# Dapatkan nama file gambar, path file, dan label satu per satu dengan looping, dan simpan sebagai dataframe\n",
        "for path, subdirs, files in os.walk(lung_path):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "distribution_train = pd.DataFrame({\"path\":full_path, 'file_name':file_name, \"labels\":labels})"
      ],
      "metadata": {
        "id": "DDLnLkJgIg4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Augmentation**"
      ],
      "metadata": {
        "id": "zEGNyy8VLrPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat fungsi untuk melakukan rotasi berlawanan arah jarum jam\n",
        "def anticlockwise_rotation(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    sudut = random.randint(0,180)\n",
        "    return rotate(img, sudut)\n",
        "\n",
        "# Membuat fungsi untuk melakukan rotasi searah jarum jam\n",
        "def clockwise_rotation(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    sudut = random.randint(0,180)\n",
        "    return rotate(img, -sudut)\n",
        "\n",
        "# Membuat fungsi untuk membalik gambar secara vertikal dari atas ke bawah\n",
        "def flip_up_down(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    return np.flipud(img)\n",
        "\n",
        "# Membuat fungsi untuk memberikan efek peningkatan kecerahan pada gambar\n",
        "def add_brightness(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    img = adjust_gamma(img, gamma=0.5,gain=1)\n",
        "    return img\n",
        "\n",
        "# Membuat fungsi untuk memberikan efek blur pada gambar\n",
        "def blur_image(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    return cv2.GaussianBlur(img, (9,9),0)\n",
        "\n",
        "# Membuat fungsi untuk memberikan efek pergeseran acak pada gambar\n",
        "def sheared(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    transform = AffineTransform(shear=0.2)\n",
        "    shear_image = warp(img, transform, mode=\"wrap\")\n",
        "    return shear_image\n",
        "\n",
        "# Membuat fungsi untuk melakukan pergeseran melengkung pada gambar\n",
        "def warp_shift(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    transform = AffineTransform(translation=(0,40))\n",
        "    warp_image = warp(img, transform, mode=\"wrap\")\n",
        "    return warp_image"
      ],
      "metadata": {
        "id": "F2aiYU5HIrYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat variabel transformasi yang akan menyimpan semua proses pra-pemrosesan yang telah dilakukan sebelumnya\n",
        "transformations = { 'rotate anticlockwise': anticlockwise_rotation,\n",
        "                    'rotate clockwise': clockwise_rotation,\n",
        "                    'warp shift': warp_shift,\n",
        "                    'blurring image': blur_image,\n",
        "                    'add brightness' : add_brightness,\n",
        "                    'flip up down': flip_up_down,\n",
        "                    'shear image': sheared\n",
        "                  }\n",
        "\n",
        "images_path=\"chest_xray/dataset/NORMAL\" # Path untuk gambar asli\n",
        "augmented_path=\"chest_xray/dataset/NORMAL\" # Path untuk gambar yang sudah diaugmentasi\n",
        "images=[] # Penyimpanan gambar yang telah melalui pra-pemrosesan\n",
        "\n",
        "# Baca nama gambar dari folder dan tambahkan path ke dalam array \"images\"\n",
        "for im in os.listdir(images_path):\n",
        "    images.append(os.path.join(images_path,im))\n",
        "\n",
        "# Jumlah gambar yang akan ditambahkan dengan hasil transformasi augmentasi, jumlahnya disesuaikan sesuai kebutuhan\n",
        "# Variabel untuk melakukan iterasi sampai jumlah gambar yang ditentukan dalam images_to_generate\n",
        "images_to_generate=2000\n",
        "i=1\n",
        "\n",
        "while i<=images_to_generate:\n",
        "    image=random.choice(images)\n",
        "    try:\n",
        "        original_image = io.imread(image)\n",
        "        transformed_image=None\n",
        "        n = 0      # Variabel untuk melakukan iterasi sampai jumlah transformasi yang akan diterapkan\n",
        "        transformation_count = random.randint(1, len(transformations)) # Pilih jumlah transformasi acak yang akan diterapkan pada gambar\n",
        "\n",
        "        while n <= transformation_count:\n",
        "            key = random.choice(list(transformations)) # Secara acak memilih dan memanggil metode\n",
        "            transformed_image = transformations[key](original_image)\n",
        "            n = n + 1\n",
        "\n",
        "        new_image_path= \"%s/augmented_image_%s.jpg\" %(augmented_path, i)\n",
        "        transformed_image = img_as_ubyte(transformed_image)  # Mengonversi gambar ke format byte yang tidak ditandatangani, dengan nilai dalam [0, 255]\n",
        "        cv2.imwrite(new_image_path, transformed_image)  # Simpan hasil transformasi augmentasi pada gambar ke path yang ditentukan\n",
        "        i =i+1\n",
        "    except ValueError as e:\n",
        "        print('could not read the',image ,':',e,'hence skipping it.')"
      ],
      "metadata": {
        "id": "ScLxGpnhItzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisikan path sumber\n",
        "lung_path = \"chest_xray/dataset/\"\n",
        "\n",
        "# Buat daftar yang menyimpan data untuk setiap nama file, path file, dan label dalam data\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "\n",
        "# Dapatkan nama file gambar, path file, dan label satu per satu dengan looping, dan simpan sebagai dataframe\n",
        "for path, subdirs, files in os.walk(lung_path):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "distribution_train = pd.DataFrame({\"path\":full_path, 'file_name':file_name, \"labels\":labels})\n",
        "\n",
        "# Plot distribusi gambar di setiap kelas\n",
        "Label = distribution_train['labels']\n",
        "plt.figure(figsize = (6,6))\n",
        "sns.set_style(\"darkgrid\")\n",
        "plot_data = sns.countplot(Label)"
      ],
      "metadata": {
        "id": "M3EsZhcAIwLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Panggil variabel mypath yang menampung folder dataset gambar\n",
        "mypath= 'chest_xray/dataset/'\n",
        "\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "for path, subdirs, files in os.walk(mypath):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "# Memasukkan variabel yang sudah dikumpulkan pada looping di atas menjadi sebuah dataframe agar rapi\n",
        "df = pd.DataFrame({\"path\":full_path,'file_name':file_name,\"labels\":labels})\n",
        "# Melihat jumlah data gambar pada masing-masing label\n",
        "df.groupby(['labels']).size()"
      ],
      "metadata": {
        "id": "EhTqzYIjI0se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variabel yang digunakan pada pemisahan data ini di mana variabel x = data path dan y = data labels\n",
        "\n",
        "X= df['path']\n",
        "y= df['labels']\n",
        "\n",
        "# Split dataset awal menjadi data train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=300)"
      ],
      "metadata": {
        "id": "PKiTZrpRI4cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyatukan ke dalam masing-masing dataframe\n",
        "df_tr = pd.DataFrame({'path':X_train,'labels':y_train,'set':'train'})\n",
        "df_te = pd.DataFrame({'path':X_test,'labels':y_test,'set':'test'})"
      ],
      "metadata": {
        "id": "NEli1H-jI7C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabungkan DataFrame df_tr dan df_te\n",
        "df_all = pd.concat([df_tr, df_te], ignore_index=True)\n",
        "\n",
        "print('===================================================== \\n')\n",
        "print(df_all.groupby(['set', 'labels']).size(), '\\n')\n",
        "print('===================================================== \\n')\n",
        "\n",
        "# Cek sampel data\n",
        "print(df_all.sample(5))\n",
        "\n",
        "# Memanggil dataset asli yang berisi keseluruhan data gambar yang sesuai dengan labelnya\n",
        "datasource_path = \"chest_xray/dataset/\"\n",
        "# Membuat variabel Dataset, tempat menampung data yang telah dilakukan pembagian data training dan testing\n",
        "dataset_path = \"Dataset-Final/\""
      ],
      "metadata": {
        "id": "W4GaHah3I89x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in tq(df_all.iterrows()):\n",
        "    # Deteksi filepath\n",
        "    file_path = row['path']\n",
        "    if os.path.exists(file_path) == False:\n",
        "            file_path = os.path.join(datasource_path,row['labels'],row['image'].split('.')[0])\n",
        "\n",
        "    # Buat direktori tujuan folder\n",
        "    if os.path.exists(os.path.join(dataset_path,row['set'],row['labels'])) == False:\n",
        "        os.makedirs(os.path.join(dataset_path,row['set'],row['labels']))\n",
        "\n",
        "    # Tentukan tujuan file\n",
        "    destination_file_name = file_path.split('/')[-1]\n",
        "    file_dest = os.path.join(dataset_path,row['set'],row['labels'],destination_file_name)\n",
        "\n",
        "    # Salin file dari sumber ke tujuan\n",
        "    if os.path.exists(file_dest) == False:\n",
        "        shutil.copy2(file_path,file_dest)"
      ],
      "metadata": {
        "id": "YJRnpFONI_ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Image Data Generator**"
      ],
      "metadata": {
        "id": "jVGMMZKpL4Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisikan direktori training dan test\n",
        "TRAIN_DIR = \"Dataset-Final/train/\"\n",
        "TEST_DIR = \"Dataset-Final/test/\"\n",
        "\n",
        "train_normal = os.path.join(TRAIN_DIR + '/NORMAL')\n",
        "train_pneumonia = os.path.join(TRAIN_DIR + '/PNEUMONIA')\n",
        "test_normal = os.path.join(TEST_DIR + '/NORMAL')\n",
        "test_pneumonia = os.path.join(TEST_DIR + '/PNEUMONIA')\n",
        "\n",
        "print(\"Total number of normal images in training set: \",len(os.listdir(train_normal)))\n",
        "print(\"Total number of pneumonic images in training set: \",len(os.listdir(train_pneumonia)))\n",
        "print(\"Total number of normal images in test set: \",len(os.listdir(test_normal)))\n",
        "print(\"Total number of pneumonic images in test set: \",len(os.listdir(test_pneumonia)))"
      ],
      "metadata": {
        "id": "V2rwlFO5JCbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat objek ImageDataGenerator yang menormalkan gambar\n",
        "datagen = ImageDataGenerator(rescale=1/255.,\n",
        "                             validation_split = 0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                              batch_size=32,\n",
        "                                              target_size=(150,150),\n",
        "                                              color_mode=\"grayscale\",\n",
        "                                              class_mode='binary',\n",
        "                                              subset='training',\n",
        "                                              shuffle=True)\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                                   batch_size=32,\n",
        "                                                   target_size=(150,150),\n",
        "                                                color_mode=\"grayscale\",\n",
        "                                                   class_mode='binary',\n",
        "                                                   subset='validation',\n",
        "                                                   shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(TEST_DIR,\n",
        "                                                  batch_size=1,\n",
        "                                                  target_size=(150,150),\n",
        "                                                  color_mode=\"grayscale\",\n",
        "                                                  class_mode='binary',\n",
        "                                                  shuffle=False)"
      ],
      "metadata": {
        "id": "2FpXXWYUJE4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model ResNet**"
      ],
      "metadata": {
        "id": "iwDddDmsNvgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout, Add, Activation, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import Input, regularizers\n",
        "\n",
        "# Clear session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "####################### Residual Block Function ##################################\n",
        "def residual_block(x, filters, kernel_size=3, stride=1):\n",
        "    shortcut = x\n",
        "    # First Conv layer\n",
        "    x = Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Second Conv layer\n",
        "    x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Shortcut connection\n",
        "    if stride != 1 or shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv2D(filters, kernel_size=1, strides=stride, padding='same')(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "####################### Build ResNet model in Sequential ##################################\n",
        "model_organ = Sequential()\n",
        "\n",
        "# Input layer\n",
        "model_1.add(Input(shape=(150, 150, 1)))\n",
        "\n",
        "# Initial Conv layer\n",
        "model_organ.add(Conv2D(32, (3, 3), strides=(1, 1), padding='same', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model_organ.add(BatchNormalization())\n",
        "model_organ.add(Activation('relu'))\n",
        "\n",
        "# First Residual Block\n",
        "model_organ.add(residual_block(model_1.output, filters=32))\n",
        "\n",
        "# Second Residual Block with downsampling\n",
        "model_organ.add(residual_block(model_1.output, filters=64, stride=2))\n",
        "\n",
        "# Third Residual Block with downsampling\n",
        "model_organ.add(residual_block(model_1.output, filters=128, stride=2))\n",
        "\n",
        "# Fourth Residual Block\n",
        "model_organ.add(residual_block(model_1.output, filters=256))\n",
        "\n",
        "# Global Average Pooling to reduce dimensions before fully connected layers\n",
        "model_organ.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Fully connected layers\n",
        "model_organ.add(Dense(128, activation='relu'))\n",
        "model_organ.add(Dropout(0.5))\n",
        "model_organ.add(Dense(64, activation='relu'))\n",
        "model_organ.add(Dropout(0.3))\n",
        "\n",
        "model_organ.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_organ.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Summary of the Model Architecture\n",
        "print(model_organ.summary())"
      ],
      "metadata": {
        "id": "cM6sqTF0NuCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # tf.keras.backend.clear_session()\n",
        "\n",
        "# ####################### Init sequential model ##################################\n",
        "# model_1 = Sequential()\n",
        "\n",
        "# # ######################### Input layer with Fully Connected Layer ################################\n",
        "# # 1st Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "# model_1.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(150,150,1)))\n",
        "# model_1.add(BatchNormalization())\n",
        "# model_1.add(MaxPool2D((2, 2)))\n",
        "\n",
        "# # 2nd Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "# model_1.add(Conv2D(32, (4, 4),padding='same', activation='relu'))\n",
        "# model_1.add(BatchNormalization())\n",
        "# model_1.add(MaxPool2D((2, 2)))\n",
        "\n",
        "# # 3rd Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "# model_1.add(Conv2D(32, (7, 7), padding='same', activation='relu'))\n",
        "# model_1.add(BatchNormalization())\n",
        "# model_1.add(MaxPool2D((2, 2)))\n",
        "\n",
        "# # Flatten layer\n",
        "# model_1.add(Flatten())\n",
        "# # 1nd Dense Layer\n",
        "# model_1.add(Dense(128, activation = 'relu'))\n",
        "# # 1nd Dropout Layer\n",
        "# model_1.add(Dropout(0.5))\n",
        "# # 2nd Dense Layer\n",
        "# model_1.add(Dense(64, activation = 'relu'))\n",
        "# # 2nd Dropout Layer\n",
        "# model_1.add(Dropout(0.3))\n",
        "\n",
        "# # Final Dense layer => For output prediction 1 mean (binary class in dataset), sigmoid for binary cases\n",
        "# model_1.add(Dense(1, activation='sigmoid'))\n",
        "# ######################### Fully Connected Layer ################################\n",
        "\n",
        "# ######################### Compile Model ################################\n",
        "# model_1.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
        "#                 loss='binary_crossentropy',\n",
        "#                 metrics=['accuracy'])\n",
        "\n",
        "# # Summary of the Model Architecture\n",
        "# print(model_1.summary())"
      ],
      "metadata": {
        "id": "ngK4SVyGJGSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah gambar untuk setiap kelas\n",
        "total_normal = len(os.listdir(train_normal))\n",
        "total_pneumonia = len(os.listdir(train_pneumonia))\n",
        "\n",
        "# Menghitung bobot kelas berdasarkan perbandingan jumlah data\n",
        "normal_weight = (total_normal + total_pneumonia) / (2 * total_normal)\n",
        "pneumonia_weight = (total_pneumonia + total_normal) / (2 * total_pneumonia)\n",
        "\n",
        "class_weights = {0: normal_weight, 1: pneumonia_weight}\n",
        "\n",
        "# Mendefinisikan callback untuk menghentikan pelatihan berdasarkan akurasi\n",
        "class StopOnAccuracy(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        accuracy_train = logs.get('accuracy')\n",
        "        accuracy_val = logs.get('val_accuracy')\n",
        "\n",
        "        if accuracy_train and accuracy_val:\n",
        "            if accuracy_train > 0.85 and accuracy_val > 0.85:\n",
        "                print(\"\\nPelatihan dihentikan: Akurasi telah mencapai 85% pada pelatihan dan validasi.\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "# Membuat instance callback\n",
        "stop_training_callback = StopOnAccuracy()\n"
      ],
      "metadata": {
        "id": "9HieUQUZQ1iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count_normal, count_pneumonia = len(os.listdir(train_normal)), len(os.listdir(train_pneumonia))\n",
        "# weight_0 = (1 / count_normal) * (count_normal + count_pneumonia) / 2.0\n",
        "# weight_1 = (1 / count_pneumonia) * (count_pneumonia + count_normal) / 2.0\n",
        "\n",
        "# class_weights = {0 : weight_0, 1 : weight_1}\n",
        "\n",
        "# %time\n",
        "\n",
        "# class myCallback(tf.keras.callbacks.Callback):\n",
        "#     def on_epoch_end(self, epoch, logs={}):\n",
        "#         if(logs.get('accuracy') > 0.85 and logs.get('val_accuracy') > 0.85):\n",
        "#           print(\"\\nAkurasi telah mencapai >85%\")\n",
        "#           self.model.stop_training = True\n",
        "\n",
        "# callbacks = myCallback()\n",
        "\n",
        "# # Fitting / training model\n",
        "# # history_1 = model_1.fit(train_generator,\n",
        "# #                         epochs=3,\n",
        "# #                         batch_size=32,\n",
        "# #                         validation_data=validation_generator,\n",
        "# #                         class_weight = class_weights,\n",
        "# #                         # callbacks=callbacks\n",
        "# #                         )"
      ],
      "metadata": {
        "id": "DGGVYmNYJNFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grafik akurasi dan loss**"
      ],
      "metadata": {
        "id": "2K5qbUPRJ57k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengambil data akurasi dan loss dari history pelatihan\n",
        "train_acc = history_1.history['accuracy']\n",
        "val_acc = history_1.history['val_accuracy']\n",
        "train_loss = history_1.history['loss']\n",
        "val_loss = history_1.history['val_loss']\n",
        "\n",
        "epoch_range = range(len(train_acc))\n",
        "\n",
        "# Plot Akurasi Pelatihan dan Validasi\n",
        "plt.plot(epoch_range, train_acc, color='red', label='Training Accuracy')\n",
        "plt.plot(epoch_range, val_acc, color='blue', label='Validation Accuracy')\n",
        "plt.title('Akurasi Pelatihan dan Validasi')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Akurasi')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss Pelatihan dan Validasi\n",
        "plt.plot(epoch_range, train_loss, color='red', label='Training Loss')\n",
        "plt.plot(epoch_range, val_loss, color='blue', label='Validation Loss')\n",
        "plt.title('Loss Pelatihan dan Validasi')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-nPkDYFBRidr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# acc = history_1.history['accuracy']\n",
        "# val_acc = history_1.history['val_accuracy']\n",
        "# loss = history_1.history['loss']\n",
        "# val_loss = history_1.history['val_loss']\n",
        "\n",
        "# epochs = range(len(acc))\n",
        "\n",
        "# plt.plot(epochs, acc, 'r')\n",
        "# plt.plot(epochs, val_acc, 'b')\n",
        "# plt.title('Training and Validation Accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'val'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(epochs, loss, 'r')\n",
        "# plt.plot(epochs, val_loss, 'b')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'val'], loc='upper left')\n",
        "# plt.title('Training and Validaion Loss')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "H2wYD8TsJUZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Akurasi ML**"
      ],
      "metadata": {
        "id": "fSWD1T59J8ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengatur ulang test generator\n",
        "test_generator.reset()\n",
        "\n",
        "# Melakukan prediksi pada test set\n",
        "predictions = model_organ.predict(test_generator, verbose=0)\n",
        "predictions = predictions.copy()\n",
        "\n",
        "# Mengonversi probabilitas menjadi label biner berdasarkan threshold 0.5\n",
        "predictions[predictions <= 0.5] = 0\n",
        "predictions[predictions > 0.5] = 1\n",
        "\n",
        "# Menampilkan laporan klasifikasi\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nLaporan Klasifikasi:\")\n",
        "print(classification_report(y_true=test_generator.classes, y_pred=predictions, target_names=['Normal', 'Pneumonia'], digits=4))\n"
      ],
      "metadata": {
        "id": "q-KyfXUPRKOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_generator.reset()\n",
        "\n",
        "# preds_1 = model_1.predict(test_generator,verbose=0)\n",
        "# preds_1 = preds_1.copy()\n",
        "# preds_1[preds_1 <= 0.5] = 0\n",
        "# preds_1[preds_1 > 0.5] = 1\n",
        "\n",
        "# # Print Confusion Matrix\n",
        "# # cm = pd.DataFrame(data=confusion_matrix(test_generator.classes, preds_1, labels=[0, 1]),index=[\"Actual Normal\", \"Actual Pneumonia\"],\n",
        "# # columns=[\"Predicted Normal\", \"Predicted Pneumonia\"])\n",
        "# # sns.heatmap(cm,annot=True,fmt=\"d\")\n",
        "\n",
        "# # Print Classification Report\n",
        "# print(\"\\n\")\n",
        "# print(classification_report(y_true=test_generator.classes,y_pred=preds_1,target_names =['Normal','Pneumonia'], digits=4))"
      ],
      "metadata": {
        "id": "Arf6ScwHJYpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Save Model**"
      ],
      "metadata": {
        "id": "C1rvavC1KGsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mendapatkan class names dari train generator\n",
        "class_indices = train_generator.class_indices\n",
        "\n",
        "# Mengubah dictionary class_indices menjadi list class names\n",
        "class_names = list(class_indices.keys())\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "nW1A4l5EKL-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model ke format SavedModel\n",
        "tf.saved_model.save(model_organ, \"saved_model\")"
      ],
      "metadata": {
        "id": "92ROvEbFKNgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mengonversi Model ke TensorFlow Lite**"
      ],
      "metadata": {
        "id": "BEq75AWtKRCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_organ.save('model_saya.keras')"
      ],
      "metadata": {
        "id": "sFZof29XKaw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Muat model yang telah disimpan dalam format .keras\n",
        "model = tf.keras.models.load_model('model_saya.keras')\n",
        "\n",
        "\n",
        "os.makedirs('tflite', exist_ok=True)\n",
        "\n",
        "# Konversi model ke format TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Simpan model TFLite ke file\n",
        "with open('tflite/model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "lCUdaHHqKgI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "id": "dofNhWwmKpKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi SavedModel ke format TFJS\n",
        "import os\n",
        "os.makedirs('tfjs_model', exist_ok=True)"
      ],
      "metadata": {
        "id": "By-UIq3vKqaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model saved_model tfjs_model/"
      ],
      "metadata": {
        "id": "lya7ypkXKuAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}